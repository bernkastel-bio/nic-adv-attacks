# nic_adv_attacks
Investigating the Impact of Adversarial Attacks on AI-Based Image Compression Models
In recent years, artificial neural networks have been widely used for their ability to quickly process large amounts of data and find non-linear dependencies in the data. Neural networks are used mainly for classification and regression tasks (detection of objects in photos and videos, financial risk analysis, and many others).  A separate niche in the types of artificial neural networks is occupied by generative models that are used to generate new data.  Their architecture consists of two key blocks: an encoder (compresses the source data) and a decoder (decompresses the processed data). This architecture is very similar to the process of compression algorithms, so reports began to appear in the scientific community about the use of convolutional neural networks in the problem of data compression. And they show quite good results, comparable in quality to current compression algorithms.
However, the main problem with applying convolutional neural networks to data compression tasks is their extreme sensitivity to external noise, which leads to their incorrect operation.
To explore the compression models adversarial attacks resistance we use Discretized Gaussian Mixture Likelihood models: Cheng2020Anchor and Cheng2020Attn and the several attack strategies.
